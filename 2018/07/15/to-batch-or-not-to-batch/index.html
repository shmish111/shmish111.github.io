<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> To Batch Or Not To Batch · Continuously Improving</title><meta name="description" content="To Batch Or Not To Batch - David Smith"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="short icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,600" type="text/css"></head><body><header><a href="/" class="logo-link"><img src="/favicon.png"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/shmish111" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><section class="container"><div class="post"><article class="post-block"><h1 class="post-title">To Batch Or Not To Batch</h1><div class="post-time">Jul 15, 2018</div><div class="post-content"><blockquote>
<p>Measure your test pipeline before trying to improve efficiency with batching</p>
</blockquote>
<p>In projects with lots of contributors and long running tests, it seems like common sense to batch features together in order to reduce the overhead of running the long tests. In this post I try to prove whether this is a valid thing to do.<br><a id="more"></a></p>
<p>The code I used for this simulation is available <a href="https://gitlab.com/shmish111/bisecting" target="_blank" rel="external">on gitlab</a></p>
<h2 id="The-Scenario"><a href="#The-Scenario" class="headerlink" title="The Scenario"></a>The Scenario</h2><p>Let’s say we have 100 developers all contributing to a large and complex code base. In order to release a new version of the system, we have to carry out a load of tests and these tests include some end-to-end tests that are pretty slow. Let’s say on average we get around 24 merge requests per day and a test run takes about an hour. In this situation it looks like we are only just going to make it, if the number of merge requests per day increases then we are going to end up with an ever increasing backlog. In order to increase the potential capacity we can batch together a few merge requests and test those. What happens if a test fails though? We can bisect the batch to find the merge request that caused the tests to fail and remove that one.</p>
<p>This was in fact a real scenario in a previous contract of mine, in this case the time it took to merge a feature varied wildly since a bisection could take a few test runs to merge a batch, or it could take just 1 test run to merge. From previous jobs I had an intuition that in addition to leveling out the lead time, running each merge request in turn would actually be quicker on average. This turned out to be more or less correct however the amazing flakiness of the tests made it difficult for people to believe since the lead time was still very high. I wanted to prove that this should always be the case, not just from a small amount of anecdotal evidence.</p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>I’m sure that someone with the correct mathematical expertise could come up with a proof however since I am not an expert, I decided to simulate the scenario instead, this should be pretty simple with some Haskell!</p>
<p>In order to avoid confusion, let’s define some terms that we are going to use in the rest of the article.</p>
<h4 id="merge-request"><a href="#merge-request" class="headerlink" title="merge request"></a>merge request</h4><p>A developer writes some code and requests that this code be merged into the master branch, this is a merge request and could take the form of a pull request, a single commit or a patch, depending on your workflow. Most people can think “merge request” == “pull request”.</p>
<h4 id="test-run"><a href="#test-run" class="headerlink" title="test run"></a>test run</h4><p>Some tests are run against the code, this is likely made up of multiple tests. If one test fails then the test run fails.</p>
<h4 id="merge-request-batch"><a href="#merge-request-batch" class="headerlink" title="merge request batch"></a>merge request batch</h4><p>A number of merge requests grouped together. As an example a common workflow is to have a <code>staging</code> branch and a <code>master</code> branch. Developers create a pull request from their feature branch into <code>staging</code>. The <code>staging</code> branch is then tested, usually a combination of end to end tests and manual tests. It might be common for there to be 5 or 6 pull requests merged into <code>staging</code> and then tested however in this simulation we will have batches of thousands in order to calculate the averages more accurately.</p>
<h4 id="failure-probability"><a href="#failure-probability" class="headerlink" title="failure probability"></a>failure probability</h4><p>We will talk quite a bit about the probability of failure. What this means is that if a developer writes some code, there is an n percent chance that this will fail a test run. You may not have thought about this before but it can quite easily be measured.</p>
<h3 id="Test-cases"><a href="#Test-cases" class="headerlink" title="Test cases"></a>Test cases</h3><p>A merge request can be modeled as a boolean value, either it passes all tests or it doesn’t. We are assuming that a test will always pass if the code is good (tests are not flaky).</p>
<p>We can then model a batch of merge requests as a list of booleans. One variable we have is how many merge requests are good and how many are bad. We can set a mean failure rate i.e. how likely is a merge request to fail? For simplicity I have used an integer for this, the percentage of merge requests that fail. We can then make a batch of merge requests by creating a list of 100 booleans where <code>n</code> are <code>False</code> and <code>100 - n</code> are <code>True</code> and then shuffle them into a random order.</p>
<p>A test run simply checks if any <code>False</code> values exist in the batch.</p>
<h3 id="Bisection"><a href="#Bisection" class="headerlink" title="Bisection"></a>Bisection</h3><p>If there is a failing merge request in the batch then we can split the batch in 2 and check each sub-batch again. Unfortunately this isn’t a simple binary search since we may have multiple failing merge requests, we must therefore run tests against a sub-batch even if the opposing sub-batch failed. If a batch of 2 tests fails and one of those tests passes by itself, we <em>can</em> assume that the other test fails (in a scenario with flaky tests we cannot make this assumption). Consider an example:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">let a = goodCode</span><br><span class="line">    b = badCode</span><br><span class="line">    c = goodCode</span><br><span class="line">    d = goodCode</span><br><span class="line">    </span><br><span class="line">[a, b, c, d] -&gt; Failed</span><br><span class="line"></span><br><span class="line">[a, b] -&gt; Failed</span><br><span class="line"></span><br><span class="line">[c, d] -&gt; Passed</span><br><span class="line"></span><br><span class="line">[a] -&gt; Passed</span><br><span class="line"></span><br><span class="line">[b] -&gt; Don&apos;t need to check</span><br></pre></td></tr></table></figure>
<p>In this case we had to run the tests 4 times in order to merge all good requests, the same number of test runs as running all merge requests individually.</p>
<h2 id="Batch-Size"><a href="#Batch-Size" class="headerlink" title="Batch Size"></a>Batch Size</h2><p>Intuition told me that bigger batch sizes would perform worse, we can model the batch size by using <code>n = batchSize</code> instead of <code>100</code>. We then need to scale the number of <code>True</code> and <code>False</code> values accordingly. We end up with a list of <code>batchSize</code> boolean values, some of them <code>True</code> and some <code>False</code>.</p>
<p>As it turns out, my intuition was wrong, batch size has no effect other than to reduce the <a href="https://www.mathsisfun.com/data/standard-deviation.html" target="_blank" rel="external">standard deviation</a>, i.e. if we run a bigger batch size we get a more accurate value of the number of test runs that will be required on average. This simplifies the code nicely.</p>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>I used <a href="http://hackage.haskell.org/package/Chart" target="_blank" rel="external">Chart</a> to plot the results and increased the batch size to increase accuracy. I am able to produce a graph of test failure probability vs the number of test runs required on average. I added a line to the plot with a constant test runs value of the batch size, this represents the situation if we don’t batch at all.</p>
<p><img src="/images/test-runs.svg" alt="test runs"></p>
<p>We can see from this chart that if 24% or more merge requests fail then it is more efficient <em>not</em> to batch our merge requests.</p>
<p>What about if our test runs are a bit flaky and they do fail occasionally when they should pass? One way to retry quite efficiently is to run tests against tests that we assume are bad because they failed when run with a test that passes by itself. We can simulate this in the following way:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">let a = goodCode</span><br><span class="line">    b = goodCode</span><br><span class="line"></span><br><span class="line">[a, b] -&gt; Failed due to a flaky test</span><br><span class="line"></span><br><span class="line">[a] -&gt; Passed</span><br><span class="line"></span><br><span class="line">-- we could assume that the right side will fail however we know that the original batch</span><br><span class="line">-- could have failed due to a flaky test, we will therefore run it anyway</span><br><span class="line">[b] -&gt; Passed</span><br></pre></td></tr></table></figure>
<p>If we make that addition to the algorithm we get the following results:</p>
<p><img src="/images/flaky-test-runs.svg" alt="flaky test runs"></p>
<p>We can see that now if 16% or more merge requests fail then it is more efficient <em>not</em> to batch our merge requests.</p>
<h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>So how does all this help us? Well, if your merge requests tend to fail less than 24% of the time then you can gain some speed by batching. 24% might seem quite high, surely the code we write doesn’t fail that often? In reality it’s not that high especially if you have loads of imperfect end to end tests. Be honest, how green is your pipeline? If your tests are a bit flaky but more than 16% of merge requests still pass on average then you can use the alternative bisection algorithm and make some gains.</p>
<p>Of course this is just a simulation and there are other factors which affect reality. It’s also worth noting that this simulation assumes that no heuristics or intelligence is used to pick out which tests are likely to be causing the failures. Think of the scenario where you use a <code>staging</code> branch and carry out tests there (possibly manual) before merging to master. In this scenario it is common for developers to inspect the test results and intelligently decide which merge requests to remove, thus increasing the chances of <code>staging</code> passing next time without having to bisect. Experience has shown me that the ability to pick out bad merge requests tends to degrade as the team grows and as the number of merge requests in staging increases since the interactions between merge requests rapidly becomes more complex.</p>
<p>We can say that whether batching is worth it or not depends only on the failure rate of merge requests and the algorithm (whether intelligent or not) used to find failing tests, it doesn’t matter how long your tests take. In the case of a simple bisection algorithm we can also say that it doesn’t matter what the batch size is either.</p>
<p>There are other reasons why you may wish to avoid batching such as reducing the risk involved in releases by deploying smaller changes. Additionally we can say that improving code quality and code reviews can allow methods of increasing efficiency such as batching. Having multiple ‘tiers’ of testing could also make batching worth it since you could decrease the likelihood of failing merge requests in a batch as other, faster tests have already passed for those merge requests.</p>
<p>The most important thing I will take away from this though is that if I am in a situation where batching is being considered (or removing existing batching) then I will make sure I spend some time measuring failure rates first before spending any engineering effort on building the test pipeline.</p>
</div></article></div></section><footer><div class="paginator"><a href="/2018/08/02/looking-at-quality/" class="prev">NEXT</a><a href="/2017/03/18/safe-json-with-haskell/" class="next">PREVIOUS</a></div><div class="copyright"><p>© 2015 - 2019 <a href="http://shmish111.github.io">David Smith</a>, unless otherwise noted.</p></div></footer><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-52255315-1",'auto');ga('send','pageview');</script><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "//hm.baidu.com/hm.js?a36e15d9e2adec9a21fcdd9f686b1ed2";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script src="https://cdn.bootcss.com/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>