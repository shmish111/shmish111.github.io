<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> To Batch Or Not To Batch · Continuously Improving</title><meta name="description" content="To Batch Or Not To Batch - David Smith"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="short icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,600" type="text/css"></head><body><header><a href="/" class="logo-link"><img src="/favicon.png"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/shmish111" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><section class="container"><div class="post"><article class="post-block"><h1 class="post-title">To Batch Or Not To Batch</h1><div class="post-time">Jul 15, 2018</div><div class="post-content"><blockquote>
<p>Only test batches of features under very specific circumstances</p>
</blockquote>
<p>In projects with lots of contributors and long running tests, it seems like common sense to batch features together in order to reduce the overhead of running the long tests. In this post I try to prove if this is a valid thing to do.<br><a id="more"></a></p>
<h2 id="The-Scenario"><a href="#The-Scenario" class="headerlink" title="The Scenario"></a>The Scenario</h2><p>Lets say we have 100 developers all contributing to a large and complex code base. In order to release a new version of the system, we have to carry out a load of tests and these tests include some end-to-end tests that are pretty slow. Lets say on average we get around 24 merge requests per day and a test run takes about an hour. In this situation it looks like we are only just going to make it, if the number of merge requests per day increases then we are going to end up with an ever increasing backlog. In order to increase the potential capacity we can batch together a few merge requests and test those. What happens if a test fails though? Well we can bisect the batch to find the merge request that caused the tests to fail and remove that one.</p>
<p>This was in fact a real scenario in a previous contract of mine, in this case the time it took to merge a feature varied wildly since a bisection could take a few test runs to merge a batch, or it could take just 1 test run to merge. From previous jobs I had an intuition that in addition to leveling out the lead time, running each merge request in turn would actually be quicker on average. This turned out to be more or less correct however the amazing flakiness of the tests made it difficult for people to believe since the lead time was still very high. I wanted to prove that this should always be the case, not just from a small amount of anecdotal evidence.</p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>I’m sure that someone with the correct mathematical expertise could come up with a proof however since I don’t have this expertise I decided to simulate the scenario instead, this should be pretty simple with some Haskell!</p>
<h3 id="Test-cases"><a href="#Test-cases" class="headerlink" title="Test cases"></a>Test cases</h3><p>A merge request can be modeled as a boolean value, either it passes all tests or it doesn’t. In reality things aren’t quite this simple, merge requests can interact with each other in complex ways and test can be flaky and fail when the merge request is OK. However as a first approximation we will consider that all our tests are perfect, we will assume that our merge requests can interact though.</p>
<p>We can then model a batch of merge requests as a list of booleans. One variable we have is how many merge requests are good and how many are bad. We can set a mean failure rate i.e. how likely is a merge request to fail? For simplicity I have used an integer for this, the percentage of merge requests that fail. We can then make a batch of merge requests by creating a list of 100 booleans where <code>n</code> are <code>False</code> and <code>100 - n</code> are <code>True</code> and then shuffle them into a random order.</p>
<p>A test run simply checks if any <code>False</code> values exist in the batch.</p>
<h3 id="Bisection"><a href="#Bisection" class="headerlink" title="Bisection"></a>Bisection</h3><p>If there is a failing merge request in the batch then we can split the batch in 2 and check each sub-batch again. Unfortunately this isn’t a simple binary search since we may have multiple failing merge requests, we must therefor run tests against a sub-batch even if the opposing sub-batch failed. If a batch of 2 tests fails and one of those tests passes by itself, we can assume that the other test fails (in a scenario with flaky tests we cannot make this assumption). Consider an example:</p>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[<span class="type">True</span>, <span class="type">False</span>, <span class="type">True</span>, <span class="type">True</span>] -&gt; <span class="type">Failed</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- left side</span></span><br><span class="line">[<span class="type">True</span>, <span class="type">False</span>] -&gt; <span class="type">Failed</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--right side</span></span><br><span class="line">[<span class="type">True</span>, <span class="type">True</span>] -&gt; <span class="type">Passed</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- left-left</span></span><br><span class="line">[<span class="type">True</span>] -&gt; <span class="type">Passed</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- left-right</span></span><br><span class="line">[<span class="type">False</span>] -&gt; <span class="type">Don't</span> need to check</span><br></pre></td></tr></table></figure>
<p>In this case we had to run the tests 4 times in order to merge all good requests, the same number of test runs as running all merge requests individually.</p>
<h2 id="Batch-Size"><a href="#Batch-Size" class="headerlink" title="Batch Size"></a>Batch Size</h2><p>Intuition told me that bigger batch sizes would perform worse, we can model the batch size by using <code>n = batchSize</code> instead of <code>100</code>. We then need to scale the number of <code>True</code> and <code>False</code> values accordingly.</p>
<p>As it turns out, my intuition was wrong, batch size has no effect other than to reduce the standard deviation, i.e. if we run a bigger batch size we get a more accurate value of the number of test runs required on average. This simplifies the code nicely.</p>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>I used <a href="http://hackage.haskell.org/package/Chart" target="_blank" rel="external">Chart</a> to plot the results and increased the batch size to increase accuracy. I am able to produce a graph of test failure probability vs the number of test runs required on average. I added a line to the plot with a constant test runs value of the batch size, this represents the situation if we don’t batch at all.</p>
<p><img src="./images/test-runs.svg" alt="test runs"></p>
<p>We can see from this chart that if 24% or more merge requests fail then it is more efficient not to batch our merge requests.</p>
<p>What about if our test runs are a bit flaky and they do fail occasionally when they should pass? One way to retry quite efficiently is to run tests against tests that we assume are bad because they failed when run with a test that passes by itself. We can simulate this in the following way:</p>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[<span class="type">True</span>, <span class="type">False</span>] -&gt; <span class="type">Failed</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- run the left side</span></span><br><span class="line">[<span class="type">True</span>] -&gt; <span class="type">Passed</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- we could assume that the right side will fail however the original batch could have failed due to a flaky test, we will therefor run it anyway</span></span><br><span class="line">[<span class="type">False</span>] -&gt; <span class="type">Failed</span> or <span class="type">Passed</span></span><br></pre></td></tr></table></figure>
<p>If we make that addition to the algorithm we get the following results:</p>
<p><img src="./images/flaky-test-runs.svg" alt="flaky test runs"></p>
<p>We can see that now if 16% or more merge requests fail then it is more efficient not to batch our merge requests.</p>
<h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>So how does all this help us? Well, if your merge requests tend to pass more than 24% of the time then you can gain some speed by batching. If your tests are a bit flaky but more than 16% of merge requests still pass on average then you can use the alternative bisection algorithm and make some gains. Either way, if you are considering batching, it would be worthwhile recording the number of failed test runs without batching for a while and see how often merge requests fail.</p>
<p>Of course this is just a simulation and there are other factors which affect reality. Its also worth noting that this simulation assumes that no heuristics or intelligence is used to pick out which tests are likely to be causing the failures. Think of the scenario where you use a <code>staging</code> branch and carry out tests there before merging to master. In this scenario it is common for developers to inspect the test results and intelligently decide which merge requests to remove, thus increasing the chances of <code>staging</code> passing next time without having to bisect. Experience has shown me that the ability to pick out bad merge requests tends to degrade as the number of merge requests in staging increases since the interaction between merge requests rapidly becomes more complex.</p>
<p>We can say that whether batching is worth it or not depends only on the failure rate of merge requests and the algorithm (whether intelligent or not) used to find failing tests, it doesn’t matter how long your tests take. In the case of a simple bisection algorithm we can also say that it doesn’t matter what the batch size is either.</p>
<p>There are other reasons why you may wish to avoid batching such as reducing the risk involved in releases by deploying smaller changes. Additionally we can say that improving code quality and code reviews can allow methods of increasing efficiency such as batching. Having multiple ‘tiers’ of testing could also make batching worth it since you could decrease the likelihood of failing merge requests in a batch as other, faster tests have already passed for those merge requests.</p>
<p>The most important thing I will take away from this though is that if I am in a situation where batching is being considered (or removing existing batching) then I will make sure I spend some time measuring failure rates first before spending any engineering effort on building the test pipeline.</p>
</div></article></div></section><footer><div class="paginator"><a href="/2017/03/18/safe-json-with-haskell/" class="next">PREVIOUS</a></div><div class="copyright"><p>© 2015 - 2018 <a href="http://shmish111.github.io">David Smith</a>, unless otherwise noted.</p></div></footer><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-52255315-1",'auto');ga('send','pageview');</script><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "//hm.baidu.com/hm.js?a36e15d9e2adec9a21fcdd9f686b1ed2";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script src="https://cdn.bootcss.com/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>